{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "478c3b76-d2ed-4b70-8acd-4e40a15ba6a9",
   "metadata": {},
   "source": [
    "# Assignment 4, task 3\n",
    "\n",
    "In this task, you are going to implement a character model based on the Transformer architecture, starting from the provided skeleton. It is useful to first do exercise 2 before starting on this task.\n",
    "\n",
    "The model you are going to implement here will have a context of 32 characters, i.e. it will consider the preceding 32 characters when estimating the probabilities of the possible character coming next. Due to the clever transformer architecture, the model will have less than 50,000 trainable parameters. As a comparison, the simpler model in exercise 2 only had a context of 8 characters but had more than 300,000 trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ab7eca6-a6b5-4325-be4c-4e0f0ad09e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run this cell\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbde199-fa08-4c83-b496-52c5b69df042",
   "metadata": {},
   "source": [
    "We need to map every type of input item (every character, in our case) to a unique ID number. Since we are not sure which characters will appear in our training text, we are going to create new IDs as we encounter new kinds of characters we haven't seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9510ea6e-4171-41e5-94c7-621d6424042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_id = {}  # Dictionary to store character-to-ID mapping\n",
    "id_to_char = []  # List to store characters in their ID ordering\n",
    "PADDING_SYMBOL = '<PAD>'\n",
    "char_to_id[PADDING_SYMBOL] = 0 \n",
    "id_to_char.append( PADDING_SYMBOL )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb6f3e4-1702-4739-916c-d2e005493697",
   "metadata": {},
   "source": [
    "We now define a class 'CharDataset' that extends the predefined 'Dataset' class.Compared to exercise 2, we will create data points in a slightly different way. \n",
    "\n",
    "The init function reads a training text and splits it up into chunks $n$ characters long. From each chunk, $n$ data points with a corresponding label will be created, as in the following example:\n",
    "\n",
    "Suppose $n=8$. From a chunk $[4,5,9,11,7,7,2,12]$ with 14 being the next character ID, the following data points and labels will be formed (0 is the padding symbol):\n",
    "\n",
    "| Data point | Label |\n",
    "|-----------:|------:|\n",
    "|[4,0,0,0,0,0,0,0] | 5 |\n",
    "|[4,5,0,0,0,0,0,0] | 9 |\n",
    "|[4,5,9,0,0,0,0,0] | 11 |\n",
    "|[4,5,9,11,0,0,0,0] | 7 |\n",
    "|[4,5,9,11,7,0,0,0] | 7 |\n",
    "|[4,5,9,11,7,7,0,0] | 2 |\n",
    "|[4,5,9,11,7,7,2,0] | 12 |\n",
    "|[4,5,9,11,7,7,2,12] | 14 |\n",
    "\n",
    "This way, the model will learn to infer the next character even if the context is shorter than $n$. This is a very useful feature, particularly in 'real' language models, where the known context often is shorter than the maximal context length.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "484e5020-20e4-4c9f-8bf1-c97feeae0afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset) :\n",
    "\n",
    "    def __init__(self, file_path, n) :\n",
    "        self.datapoints = []\n",
    "        self.labels = []\n",
    "        chars = []\n",
    "        try :\n",
    "            # First read the dataset to find all the unique characters\n",
    "            with open(file_path,'r',encoding='utf-8') as f :\n",
    "                contents = f.read()\n",
    "            # YOUR CODE HERE\n",
    "            for char in contents:\n",
    "                if char not in char_to_id:\n",
    "                    char_to_id[char] = len(id_to_char)\n",
    "                    id_to_char.append(char)\n",
    "                chars.append( char_to_id[char] )\n",
    "            # Then go through all the chars and chunk them up into datapoints\n",
    "            k = 0\n",
    "            while k < len(chars)-n:\n",
    "                for i in range(1, n+1):\n",
    "                    self.datapoints.append([c for c in chars[k:i+k]+[0]*(n-i)])\n",
    "                    self.labels.append(chars[i+k])\n",
    "                k += n\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.datapoints)\n",
    "\n",
    "    def __getitem__(self, idx) :\n",
    "        idx = idx % len(self.datapoints)\n",
    "        return torch.tensor(self.datapoints[idx]), torch.tensor(self.labels[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e657a49b-9609-483a-a205-9533e933de4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Verify\n",
    "dataset = CharDataset('HP_book_1.txt', 32) # Max context is 32 characters long.\n",
    "d63,l63 = dataset[63]\n",
    "d1048575,l1048575 = dataset[1048575]\n",
    "d1048576,l1048576 = dataset[1048576]\n",
    "print(d63[16].item() == l63.item())\n",
    "print(d63[4].item() == l1048575.item())\n",
    "print(sum(d1048576[1:]).item() == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb3c12c-bc54-45ce-b5f6-cf91816499f8",
   "metadata": {},
   "source": [
    "The __self-attention__ computation is at the core of the Transformer architecture. It is important to get this computation efficient (i.e. vectorized), since it involves many matrix operations that would be very slow if implemented by Python loops.\n",
    "\n",
    "The input to the self-attention computation is a tensor containing a vector for each input token, and the output is a tensor of the same dimensions, containing the contextualized versions of the input tokens (see Lecture 9 and the textbook, chapters 10.1 and 10.2).\n",
    "\n",
    "Your task is to fill in the missing pieces below. Look for \"REPLACE WITH YOUR CODE\" and \"YOUR CODE HERE\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea2dea08-54af-4ee5-b8b6-a945687dfc08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculates self-attention according to [Vaswani et al., NeurIPS, 2017]\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, number_of_attention_heads):\n",
    "        super().__init__()\n",
    "        # The number of attention heads cannot be more than the hidden size\n",
    "        assert hidden_size > number_of_attention_heads\n",
    "        \n",
    "        self.number_of_attention_heads = number_of_attention_heads\n",
    "        # Divide the hidden_size roughly equal over the different heads\n",
    "        self.attention_head_size = int(hidden_size / number_of_attention_heads)\n",
    "        self.all_head_size = number_of_attention_heads * self.attention_head_size\n",
    "\n",
    "        # Mapping from input to the query, key, and, value vectors\n",
    "        self.query = nn.Linear(hidden_size, self.all_head_size, bias=False)\n",
    "        self.key = nn.Linear(hidden_size, self.all_head_size, bias=False)\n",
    "        self.value = nn.Linear(hidden_size, self.all_head_size, bias=False)\n",
    "\n",
    "        self.final = nn.Linear(self.all_head_size, hidden_size, bias=False)\n",
    "\n",
    "\n",
    "    def reshape_for_multihead_attention(self, x):\n",
    "        # x has the shape (batch_size, seq_length, hidden_size)\n",
    "        B,S,_ = x.shape\n",
    "\n",
    "        # but we want to split the representation of each token into 'number_of_heads' parts:\n",
    "        x = x.reshape(B,S,self.number_of_attention_heads,self.attention_head_size)\n",
    "\n",
    "        # and treat each part separately. Thus, we need the final tensor to have shape\n",
    "        # (batch_size, number_of_heads, seq_length, attention_head_size)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        # All of the tensors below will have the shape (batch_size, seq_length, hidden_size)\n",
    "        query_all_heads = self.query( hidden_states )\n",
    "        key_all_heads = self.key( hidden_states )  \n",
    "        value_all_heads = self.value( hidden_states ) \n",
    "\n",
    "        # All of the tensors below will have the shape (batch_size, number_of_heads, seq_length, attention_head_size) \n",
    "        Q = self.reshape_for_multihead_attention( query_all_heads )\n",
    "        K = self.reshape_for_multihead_attention( key_all_heads )\n",
    "        V = self.reshape_for_multihead_attention( value_all_heads )\n",
    "\n",
    "        # attention_scores will have the shape(batch_size, number_of_heads, seq_length, seq_length)\n",
    "        attention_scores = None # REPLACE WITH YOUR CODE\n",
    "\n",
    "        # Scale to reduce variance\n",
    "        attention_scores = None # REPLACE WITH YOUR CODE\n",
    "\n",
    "        # Use softmax to turn the attention scores into probabilities.\n",
    "        # We want zero scores to be zero probabilities -- hence we turn\n",
    "        # zero scores into -infinity before the softmax exponentiation.\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Now produce the contextualized vectors for each head\n",
    "        # The tensor below will have shape (batch_size, number_of_heads, seq_length, head_size)\n",
    "        self_attention_all_heads_separately = None # REPLACE WITH YOUR CODE\n",
    "\n",
    "        # For each token, we now want to bring together the representation coming from each head.\n",
    "        # The 'self_attention' tensor below should have the shape \n",
    "        # (batch size, seq_length_, self.all_heads_size)\n",
    "        self_attention = None # REPLACE WITH YOUR CODE\n",
    "\n",
    "        # Finally, make sure that the output has the correct dimensions (batch_size,seq_length,hidden_size)\n",
    "        return self.final( self_attention )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42feb356-82ee-4370-99de-23dc153ddc62",
   "metadata": {},
   "source": [
    "After the self-attention computation, the Transformer encoder block contains layer-normalization computations and a feed-forward layer. The code is given below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf90f50c-8485-4e83-a7b4-670c643b5815",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFFN(nn.Module):\n",
    "    \"\"\"\n",
    "    The position-wise FFN that follows after the self-attention\n",
    "    computation.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, dropout_prob) :\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        for module in (self.fc1, self.fc2):\n",
    "            nn.init.kaiming_normal_(module.weight)\n",
    "            nn.init.constant_(module.bias, 0.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(torch.relu(self.fc1(x))))\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder block.\n",
    "    \n",
    "    This version differs from the original version in  [Vaswani et al. NeurIPS 2017],\n",
    "    and applies the LayerNorm before the self-attention, and before the FFN, as this\n",
    "    has proved to be beneficial (see [Nguyen and Salazar 2019]).\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, number_of_attention_heads, dropout_prob) :\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadSelfAttention(hidden_size, number_of_attention_heads)\n",
    "        self.ffn = PositionwiseFFN(hidden_size, dropout_prob)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.ln1(x)\n",
    "        x2 = x + self.dropout(self.attn(x1))\n",
    "        x3 = self.ln2(x2)\n",
    "        x4 = x2 + self.dropout(self.ffn(x3))\n",
    "        return x4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edefcae-564e-4a79-8724-42f7f218f4e7",
   "metadata": {},
   "source": [
    "Here is the actual character-based language model, which uses the Transformer implementation above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c540c7d3-04e8-4ccd-907c-75b7b3d999e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Hyper-parameters for training ============== #\n",
    "\n",
    "class Config :\n",
    "    number_of_transformer_encoders = 1\n",
    "    number_of_attention_heads = 1\n",
    "    hidden_size = 64\n",
    "    dropout_prob = 0.1\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.0003\n",
    "    weight_decay = 0.000001\n",
    "    no_of_epochs = 100\n",
    "\n",
    "MAXLEN = 32   # This is the number of characters we will consider when \n",
    "              # predicting the next character\n",
    "\n",
    "# ======================= The model ======================= #\n",
    "\n",
    "class CharLM(nn.Module) :\n",
    "\n",
    "    def __init__(self, config, no_of_input_chars ) :\n",
    "        super(CharLM, self).__init__()\n",
    "        self.config = config\n",
    "        self.embed = nn.Embedding(no_of_input_chars,config.hidden_size)\n",
    "        # Make sure that the padding symbol (which has ID 0) is embedded\n",
    "        # as a vector of 0s.\n",
    "        self.embed.weight.data[0].fill_(0)\n",
    "        self.positional = nn.Parameter(torch.randn(1, MAXLEN, config.hidden_size))\n",
    "        modules = [EncoderBlock(config.hidden_size, \n",
    "                                config.number_of_attention_heads,\n",
    "                                config.dropout_prob) for _ in range(config.number_of_transformer_encoders)]\n",
    "        self.transformers = nn.ModuleList(modules)\n",
    "        self.final = nn.Linear(config.hidden_size*MAXLEN, no_of_input_chars)\n",
    "\n",
    "    def forward(self,x) :\n",
    "        number_of_datapoints = x.shape[0]\n",
    "        # First create a mask distinguishing 0 from positive word IDs\n",
    "        non_zero_mask = (x != 0)\n",
    "        word_embeddings = self.embed(x)\n",
    "        # Add positional vectors in all non-padded positions\n",
    "        pos = self.positional.expand_as(word_embeddings)\n",
    "        pos = pos * non_zero_mask.unsqueeze(-1).float()\n",
    "        t = word_embeddings + pos\n",
    "        # Then apply the transformers and make a final prediction at the end\n",
    "        for transf in self.transformers :\n",
    "            t = transf(t)\n",
    "        flattened_transf = t.reshape(number_of_datapoints,1,-1)\n",
    "        result =  self.final(torch.tanh(flattened_transf))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe7817d-f56f-465c-8499-aacc4298889b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n",
      "There are 442720 datapoints and 81 unique characters in the dataset\n",
      "15:29:27 Training starts\n",
      "15:29:43 End of epoch 1 , loss= 2.3251893520355225\n",
      "  ann andg  aatr yth iinn ggat  aarryy y aandn  atti  there yne t ana d tthe  iarer ryyan  toand t ant herr eryya  anre dyand gne d oaren dan t the eand  toha  thoee  there yta n ona t the want  thhee  arerr yane  gant  thoee tr anant  thoe  ante  arryy the aerry leand  toone gde t oou tnhe  theee t\n",
      "15:29:58 End of epoch 2 , loss= 2.2371163368225098\n",
      " sing and to the the the tore tore y and and to at warey san the ary to and ary cound the fe ary, an ary as and and to ary ary and sary of and sore and the the to the ary ton the the tore toun the sare an sary and an and ag at ory and the the to the ary ton the the tore toun the sare an sary and an \n",
      "15:30:14 End of epoch 3 , loss= 2.2614011764526367\n",
      " and and Hary and ar and and the fin to fon the the the fing the wary an's an sand an the and the fing at wary wan sand an and and the fing the the wat ore wand the se and the fing going the the to we per and to and the and to the fing the the fore y sore and and to ale son the and the was to fone w\n",
      "15:30:30 End of epoch 4 , loss= 2.113257884979248\n",
      " and and and and wary and the pery at sare ally an sat and and and and tore an wary sat and and sary an and and and and on fin fe forey salley and an and an and and the fore y soung store an wary coun and the poing the and to at and the poing the fore sat and and at and and the poing the per and tor\n",
      "15:30:45 End of epoch 5 , loss= 2.164738178253174\n",
      " and and and wary and and sat peard and fing and the sting the poing the the pomed and tore the the said and and to for and and sat the and to fing and the wary sat and and sat to fery as and and to frey an and and to frey and and and ofery fing an and the was quing the poung the fore and the se pou\n",
      "15:31:01 End of epoch 6 , loss= 2.0244436264038086\n",
      " and and and and ward and ofe the said and and the poing the the said and and to for and and the was pored and and the sting the poing the the said Harry fore and and said Harry fury and and and and and wary said and and to the fore stilly an said said Harry, Par and and said Harry furing and and to\n",
      "15:31:16 End of epoch 7 , loss= 1.9076539278030396\n",
      " ank and to for and and the was pot and ther agrid Harry ound and and saidn't and Por said Harry fard and and and saidn't say said Harry. P\"one surey said and said and Harry, and and and and and was walley and and the was pored and and the stilley said and and the poing ther agrid Harry, and and and\n",
      "15:31:32 End of epoch 8 , loss= 2.050403356552124\n",
      " ank and to for Macke and the said Harry for and and saidn't and Pore as and and and the stille as for and and they and and to for and they said and and and to for saidd and and the was and and the poing to the come and to the said and and to for and and said Harry. Pore as and and and said Ron the \n",
      "15:31:47 End of epoch 9 , loss= 1.8985649347305298\n",
      " sid and and saidn't a the fall of the was pull and and said Harry, Porfer as and said and and Ron to fe the yould as and and and to for the stally for Malley. \"Yer, as and and and the still be and the poing the said the and to the and the was quitce and to for Malley. \"I was and and the said Harry,\n",
      "15:32:03 End of epoch 10 , loss= 1.870638132095337\n",
      " some and the pore form and the saide and and the poing the for the said and and to from to stake the was quitce, Mally of the said and and to from to stake the was quitce, Mally of the said and and to from to stake the was quitce, Mally of the said and and to from to stake the was quitce, Mally of \n",
      "15:32:18 End of epoch 11 , loss= 1.7697484493255615\n",
      " sidn't he beat so for and and the poing as and and the poing the poing ot and the said Harry form and and stalley and to staid Harry form and and as for Mlack and said and and the poing and and the saide poing and the stile the said Harry quiked and of the could at the said Harry formione said stul\n",
      "15:32:33 End of epoch 12 , loss= 1.8703961372375488\n",
      " Ron, and the poing the and the was quitter and the staid some and the was looked a the quitter for Malley, and the poing the poing the the poing the stome for the for the was poing and the said Harry quike as a squille formione yeard and the some gotter and the Gryffing of the stome the and some po\n",
      "15:32:49 End of epoch 13 , loss= 1.7295596599578857\n",
      " sidn't and and staid Harry. \n",
      "\n",
      "\"They as quille and said staill and the stile and as for and stall and and staing and the still of the was leaked quutter the form and the still asking and the said and Harry form. \n",
      "\n",
      "\"It's and and some and the said Harry. \n",
      "\n",
      "\"It's and and Harry. \"Yeh was and and and the\n",
      "15:33:04 End of epoch 14 , loss= 1.8103108406066895\n",
      " stille and the still beat the pulled and the pulled and and the was poing the for the poing the and the poing and the poing the for the poing the and the poing and the poing the for the poing the and the poing and the poing the for the poing the and the poing and the poing the for the poing the and\n",
      "15:33:20 End of epoch 15 , loss= 1.8607732057571411\n",
      " sidn't a just all as and the pulled and they fill and the stome as quille and the still as and and some and the forge to the for the for the form and to stor Mally. \n",
      "\n",
      "\"It's and and Harry and as and for and they and and to for the form and to stor Mally. \n",
      "\n",
      "\"It's and and Harry and as and for and they\n",
      "15:33:35 End of epoch 16 , loss= 1.6970324516296387\n",
      " staing and the still be a sturning and the poing the going the the some and the poing the and the poing the and the stile poing and the staid said and Harry, for and sturning to studing to the said and and Ron and the poing the for the stull stuning as and the poing the for the stull stuning as and\n",
      "15:33:51 End of epoch 17 , loss= 1.7626440525054932\n",
      " sidn't and and stown as quittile and the stome of the studer they poing and and stome the stor and and the poing the said they and and and of the stilled and staid Harry, for and a stilled and Ron and the stoped and and they and Ron. \n",
      "\n",
      "\"Yere and said Harry, Profess and stor Malfey staid sudill and \n",
      "15:34:06 End of epoch 18 , loss= 1.6527519226074219\n",
      " Ron, and the going to the some and the stor and and stor McGonagall of the sturning to stoped and just the for the for Mr. \n",
      "\n",
      "\"It's and and Proffey, and Ron, the still staing and the said the for the poing the staid Ron, and the for the for the for the just all as for the fir the for stome and and s\n",
      "15:34:21 End of epoch 19 , loss= 1.6892439126968384\n",
      " Ron, and the still as and the said and a quitter and the stome of the form to the plate a staid and a the poing a the for the for the jooking the poing the staid Ron and the poing the for the forged the still of the staid a quitter the for the for Malfey, was and and and Gryfind a said and a quitte\n",
      "15:34:37 End of epoch 20 , loss= 1.7341638803482056\n",
      " sidn't and and staid Harry. \n",
      "\n",
      "\"They said Harry, found and Harry for agall and the poing the staid find the poing the poing the the some and and the poing to the and the poing the and of the for the fir stower and the poing to and the was quitter and Gryffing and Harry, fill asked and the find and s\n",
      "15:34:52 End of epoch 21 , loss= 1.6572009325027466\n",
      " staid Ron and the for the forming the poing the said the purped and and the poing the poing to the some and a sto the portle the fire and there purped of and and the poing to the the said and the poing the and the poing the and the poing the and the poing the and the poing the and the poing the and\n",
      "15:35:07 End of epoch 22 , loss= 1.7842508554458618\n",
      " staid Ron. \n",
      "\n",
      "\"He as way, and and staid Harry was and a quitte and at the for of the stor the purped and and to form to the staid the was quitter and Gryffing and Harry and as and the poing the for the poing the and and some and the poing to the fir the forge the some to the pulled and the poing to \n",
      "15:35:23 End of epoch 23 , loss= 1.6345710754394531\n",
      " staid Ron. \n",
      "\n",
      "\"He all of Quirrell as full But and the stowed a the going them and the was leate and the poing the poing the the stor McGonagall and they said Harry quilled as and the put the said and at the purped and and the forge the the poing the poing the for the poing the and the poing the and \n",
      "15:35:38 End of epoch 24 , loss= 1.6994109153747559\n",
      " staid Ron. \n",
      "\n",
      "\"He all all said Harry, \"said Harry, \"said Harry, and Ron.\" \n",
      "\n",
      "\"He said Ron. \n",
      "\"You said Harry for and staid Harry for and and a sturning and the some and and Gryffing and and staid Harry walked and a sturning the some to for the for the for the just all and said and Ron and again, \"said\n",
      "15:35:54 End of epoch 25 , loss= 1.6298483610153198\n",
      " staid Ron. \n",
      "\n",
      "\"He as way and and Poffere you stor and the stuer and and the poing the and the was and the poing to the the said Harry, poing and and said Malfoy, \"Madd and Harry quite and all some and staid Harry, fate and Harry, pearted and Gryffing and the store poing the the gotter and the fire a\n",
      "15:36:09 End of epoch 26 , loss= 1.6793789863586426\n",
      " staid Ron. \n",
      "\n",
      "\"He said Ron quite and and stapped and the for the first the place, and Ron, and and a staye said Harry. \n",
      "\n",
      "\"Just as and Ron, \"said Harry, quite and and Harry for agall the the purst it and and some Malfoy, \"said Harry, quite and and some Malfey, and the said Harry, fired and and staid \n",
      "15:36:24 End of epoch 27 , loss= 1.6396023035049438\n",
      " stay staid Harry. \n",
      "\n",
      "\"But said Harry, Poffere Mall But the some and the Gryffindore, Malfe. \n",
      "\n",
      "\"It's as and Ron. \n",
      "\n",
      "\"It's as and Harry and Ron as and a store put and the Gryffing and and store poing at and the some as and the for the studder the pearted and stue poing the and of the stope the couldn't\n",
      "15:36:40 End of epoch 28 , loss= 1.6773326396942139\n",
      " staid Ron and a suddent the poing the and to the for the first and staid Ron and agre and the staid a staid Ron and again, the a staid staid Harry. \n",
      "\n",
      "\"It's and a suddent the for the pearing to the purped and and to fire and and staid Harry. \n",
      "\n",
      "\"It's and a suddent the for the pearing to the purped an\n",
      "15:36:55 End of epoch 29 , loss= 1.8479002714157104\n",
      " stark noward Harry, \"said Harry, and Ron. \n",
      "\n",
      "\"It's all the said Harry, \"Yes,\" said Ron. \n",
      "\n",
      "\"But said Pevery Mists Gryfing and and some Mr. Notille as some and and the pearted at the poing to the poing them and the staid Harry. \n",
      "\n",
      "\"There as purst all asked out the put and the poing them and to studder \n",
      "15:37:10 End of epoch 30 , loss= 1.6578510999679565\n",
      " she specering to and the poing the and to the for the just to feace and the got pull and the pasted the and to the for the poing the and to the said the putter and the poing to the the poing the the poing the and and as quite to for the for the pearted just to pearted at and the poing the the poing\n",
      "15:37:26 End of epoch 31 , loss= 1.662822961807251\n",
      " she specering and the going there poing to the poing them and the poing to at staid the purple and the poing there and the poing there poing to the poing them and the poing to at staid the purple and the poing there and the poing there poing to the poing them and the poing to at staid the purple an\n",
      "15:37:41 End of epoch 32 , loss= 1.7712186574935913\n",
      " shought, \"Mady said Harry. \n",
      "\n",
      "\"It's said Harry, and and to studer, and the poing to the and the got the going the poing the the for the pull all and said and a puning to stome the the for the pulled and the poing to and and the store poing the poing the the for the pull all and said and a puning to \n",
      "15:37:56 End of epoch 33 , loss= 1.6473603248596191\n",
      " staid Ron. \"It and supped at and the pinter the stopped and and the for the for the for the just and a stopped and the poing to the store the plate and the poing the poing the the for the pearted and the pointing at and the pointing to stor and said Harry, \"Ron, and Harry. \n",
      "\n",
      "\"It's said Harry, and R\n",
      "15:38:12 End of epoch 34 , loss= 1.726561427116394\n",
      " staying and supped a the the for be the for the just all and to staid Ron ard Harry, squied at and staid fired a stue and the poing them and they said Harry. \n",
      "\n",
      "\"It's and said Harry, Malfoy, \"said Harry, and Ron around staid Harry, Pofess and at the plaintione a store the pull and all and said a sta\n",
      "15:38:27 End of epoch 35 , loss= 1.659181833267212\n",
      " she speced and the for the just all all them a squite all all the pare and Harry, for and as all the for the some a stopped the the for the for going the poing the staid Ron. \n",
      "\n",
      "\"It's and Potter, Mistione Ron, \"I said Harry quite as some the purst for McGonagall the got them and and the poing them a\n",
      "15:38:42 End of epoch 36 , loss= 1.7363022565841675\n",
      " sidn't say said Harry for and and as squied the Gryffindore; Harry, the some as and the poing the the poing to the stopped and at the plate and the poing to the the poing the the for studding to the poing the the for the for pup fore and the couldn't a stood the pearted to the poing to the pace and\n",
      "15:38:58 End of epoch 37 , loss= 1.7463107109069824\n",
      " staying and and to store the purned and and the poing to the stor the pulled and the pointing at said Harry, Pofess, Malford and supped and to the poing to at staid and to Malfoy, \"I and said Ron and a stay. \n",
      "\n",
      "\"Ke age there But was fulled and the poing to at said and to store.\" \n",
      "\n",
      "\"Malley, said Harr\n",
      "15:39:13 End of epoch 38 , loss= 1.7382508516311646\n",
      " she speced and and to the for the poing there staid and way poing the poing them and a said quite at aske the for Malfoy. \n",
      "\n",
      "\"Yes, as and Harry, \"said Harry, and Ron. \n",
      "\n",
      "\"Malley said Harry, Pofess, Malley as for staid Ron as quizey started at for Flull and at the poing there and them asked at the poi\n",
      "15:39:28 End of epoch 39 , loss= 1.5885093212127686\n",
      " stark now the poing the poing to at said Harry. \n",
      "\n",
      "\"It's and and Harry, pointing and some studding the poing to the come and and the poing to the schole. \n",
      "\n",
      "\"It's and the poing to at anything to staid staid Harry. \n",
      "\n",
      "\"Well,\" said Harry, and and the staid the poing to the stanter Malfoy, and Harry form\n",
      "15:39:44 End of epoch 40 , loss= 1.7539892196655273\n",
      " starking to studdent the poing the poing to the and the Gryffing all and said Harry, and and them and to stopped at the fired and a struck and a the pulled and a store the poing the and and the was one and the Gryffindore, Madam Pofess, Malford Frefully, and a staid the purst for the for the joked \n",
      "15:39:59 End of epoch 41 , loss= 1.6371008157730103\n",
      " starking to stud and a for the points the for the just all asked on the purst pulled and the purpled and the poing to the store pull for the and the poing to the schole. \n",
      "\n",
      "\"It's and the pulled and see way, \"Yes, and Ron as and Harry, \"said Harry. \n",
      "\n",
      "\"Yes, as and a studdent the poing to at staid stud\n",
      "15:40:14 End of epoch 42 , loss= 1.7269115447998047\n",
      " starking to studding the poing the and the poing the and and supped at Malfoy, they said Harry. \n",
      "\n",
      "\"Just as speard, Malfoy, \"said Ron, \"said Harry, and age and Ron as squied Gryffindore; Harry, fame studd a studer, and the stopped and the proking the the poing there poing the poing the the for the w\n",
      "15:40:30 End of epoch 43 , loss= 1.7081823348999023\n",
      " starking the purped and and them and just the pearted and the prom Mr. Norlione and said Ron, and Harry, was and squie for the staid staid Harry, was and the was and and the pearted and the points the and the poing to and the poing the pointing them. \n",
      "\n",
      "\"You said Potter, and the pulled and the poing\n",
      "15:40:45 End of epoch 44 , loss= 1.684314250946045\n",
      " see the poing the for the pulled and the poing to the staid the poing to and the poing the poing there be they and and the poing they said Ron. \n",
      "\n",
      "\"All and the was quittering to staid Gryffing and the stopped at the pace and the Gryffindore way, staid Harry, was and and said: \"Well suddent a stopped\n",
      "15:41:01 End of epoch 45 , loss= 1.719486951828003\n",
      " staying and them and Ron and and started them for Look Mall am and they said and a quittering to staid the was quitter Flull all as they was and the for the just all and the poing to and the poing to there and the poing to there staid quite at all they forge the poing there be they for and and they\n",
      "15:41:16 End of epoch 46 , loss= 1.6517688035964966\n",
      " see store placked and the pulled and and them. Gorge way and to them and to studded and the poing to the the poting to and and anything just the poing to and the poing them and a quittering to still the pull and and them anything and the staid for got the got to the got them. \n",
      "\n",
      "\"It's and them was a\n",
      "15:41:32 End of epoch 47 , loss= 1.7498283386230469\n",
      " stared them. \n",
      "\n",
      "\"It's and them to a stood the Gryfing to and staid Harry, Ron and as and the parce. \n",
      "\"\n",
      "Harry, just all as and the stopped and the poing to the staid the plate and them but and the poing to the staid them. \n",
      "\n",
      "\"Mrady, was and the got the got the just to the place and to them. He was and\n",
      "15:41:47 End of epoch 48 , loss= 1.642714500427246\n",
      " see still and the couldn't all them. \"You said Harry, form and the come a stor Mr. Norrill at and Ron and and and them and just the for the for the just all and the pearted and them. \"It's suddenly pearted a staid the was quite and the proked and the pecked and the poing there of the just as for th\n",
      "15:42:03 End of epoch 49 , loss= 1.7246241569519043\n",
      " stared the got Malfoy. \n",
      "\n",
      "\"It's all the pull and a started and the poing to the sched and and the poing there be the pull and and them and and some and the poing the and the poing the and one way, and Harry, squesting all the said and the poing them and the got the poing there of the just and and th\n",
      "15:42:18 End of epoch 50 , loss= 1.763366460800171\n",
      " starking to studding to the for the staid Harry, was and and the gried and and to the for the for the just and all tooking to the poing the and the poing the poill the for the for the just and all tooking to the poing the and the poing the poill the for the for the just and all tooking to the poing\n",
      "15:42:33 End of epoch 51 , loss= 1.8056329488754272\n",
      " see still and a struck and a got the poing them. \n",
      "\n",
      "\"Not -- we was pulled and they speaking and the points they was and and and they studding to stud and the pointe the start.\" \n",
      "\n",
      "\"It's good the pearted and Gryfing and the got the poing to at the poing to the plack of and the poing them and and and t\n",
      "15:42:49 End of epoch 52 , loss= 1.5996959209442139\n",
      " stared the for Malfoy, and and started a the Gryffing and and started and the poing to the staid the for the for studdent the poing to be the pull at for the poing them and and them and as to the poing to and the poing them and and them and as to the poing to and the poing them and and them and as \n",
      "15:43:04 End of epoch 53 , loss= 1.827830195426941\n",
      " stark they said the poing to the staid the was quite the for the pace of the pulled and they said form and the was and the way, and Harry, squest the joints and suddenly. \"It's said Ron and Harry, they stard all forge the got the poing the and and the poing the pace on the past and Ron, and Harry, \n",
      "15:43:19 End of epoch 54 , loss= 1.6765128374099731\n",
      " stard the pointing Ron, and a strue and to the some and the pulled and a stopped and the the for for stanting to staid and Gryfing to Harry, was and and said Harry, Pottering and the pointing Ron. \n",
      "\"All said Harry, and and squied the started a stopped the and the pulled and the pulled and a studder\n",
      "15:43:34 End of epoch 55 , loss= 1.7672041654586792\n",
      " stared the poing the and of the got the packed quite to and the started a the for the for forge the and and the poing to the staid and Harry, \"said Harry, said Harry, Pofessor McGonagall and and them and Harry, squestione as staid Dumbledore and said Harry, and again, they was and the poing to and \n",
      "15:43:50 End of epoch 56 , loss= 1.6558241844177246\n",
      " stared the poing the and of the got the placked out the pull and a strue and a studdenly. \n",
      "\n",
      "\"It and the pulled and the pull and a struck and as the plainting there be to puning the poing the poing the poing there be the pull and and the poing there be to studdenly and again. \"It's suddent the peart\n",
      "15:44:05 End of epoch 57 , loss= 1.6570193767547607\n",
      " stared the poing the and the poing the and and supped to the for the for Malfoy, \"said Harry, standing and the poing the poing the the for the for poing and and suddent the poing the poing the and and said Harry, and Ron, and Harry said Harry, and Ron and supped and the poing the poing the and and \n",
      "15:44:21 End of epoch 58 , loss= 1.7739295959472656\n",
      " stared the the Malfoy, was was and the grue and a staid Ron. \n",
      "\n",
      "\"Harry, squite at a staid the poing to all the poing them and the got of the come one the was and the pointing and the poing them. \n",
      "\n",
      "\"It's gaid the pulled at staid the got the got pother the for the for Malfoy, staid and again. Ron got \n",
      "15:44:36 End of epoch 59 , loss= 1.6247143745422363\n",
      " started and the points all and the was and a stopped and the the be the come one and the poing the poing the poing the poing the poing the poing the poing the poing the poing the poing the poing the poing the poing the poing the poing the poing the poing the poing the poing the poing the poing the \n",
      "15:44:51 End of epoch 60 , loss= 1.765580177307129\n",
      " suddenly poing the staid the pull and and them and them and and to started at the poill of and the poing them and and them and aroke and the poing the over Voldemort Mr. Fill and and the way stanting the purped and and they and Ron and Harry, staned as and Harry, and Ron, way, side as suddenly quit\n",
      "15:45:07 End of epoch 61 , loss= 1.6566946506500244\n",
      " see storing at Malfoy, and the pulled a said and the poing they and the poing the packed and the pointion. \n",
      "\n",
      "\"All all said Harry, and Ron all saying and the stared, but and the to stopped -- Chole all and around the a speaking quite to the Gryffindore pointing the poing Every anything just and a qu\n",
      "15:45:22 End of epoch 62 , loss= 1.7642838954925537\n",
      " see the points; the plate and the poing the points and something as staid a Professor McGonagall them and supped a quite to the Gring at suppped a the and the Gryffindor the stanting the points a staid Harry, and Ron. \n",
      "\n",
      "\"Mry, said said a quite for Malfoy, stanting to studdenly and a stopped at the \n",
      "15:45:37 End of epoch 63 , loss= 1.7487380504608154\n",
      " stared the for Malfoy, standing Ron. \n",
      "\n",
      "\"Well the was quite to the started and the poing to the staid the pulled a puning to the poing the past and the pointione a quite of the Gryffindor fame Look the stant the plate and Gryffindor the stant Ron, and Harry, stance, Malfoy, and the just a staid; a q\n",
      "15:45:53 End of epoch 64 , loss= 1.6194976568222046\n",
      " stared the got Malfoy, was was and they said and Harry, said Harry, and and the stopper and the poing to the staid the poing to at was the for the pace and to the pointion. \n",
      "\n",
      "\"All anything and Ron said Harry, squied at Potter, Malfoy, and Harry, was and and they said a Poomfed a But was stard the p\n",
      "15:46:08 End of epoch 65 , loss= 1.820261836051941\n",
      " stared the got Malfoy, was was all and the going the poing them. \n",
      "\n",
      "\"It was get one got the got the spection. \n",
      "\n",
      "\"Mrriding and said a strugh the just the points of the stopped and the pointing Ron and Harry, said Harry, and and them got the going the pointing and But Harry, and Ron and said: \"He sudd\n",
      "15:46:23 End of epoch 66 , loss= 1.6623488664627075\n",
      " stared the got Malfoy, and the stare stard again. \n",
      "\n",
      "\"They suddent all put a pulled a they and a poing to the the poing the poing them. \n",
      "\n",
      "\"It's got and the got poing the poing the poing to the start.\" \n",
      "\n",
      "He pulled and and the was but a they was and they for and a stopped the and the purped of the poi\n",
      "15:46:39 End of epoch 67 , loss= 1.7255371809005737\n",
      " see still and and the was quite and a quite to the the poing to the studdent and the pointion. \"I said Harry, and Ron and said Harry, squid Weasley and the poing to the cornight and see and the pointing the start.\" \n",
      "\n",
      "Hermione and suddenly and Ron pulled a said Harry, and and them and Ron and the st\n",
      "15:46:54 End of epoch 68 , loss= 1.6148827075958252\n",
      " stared and the seemed and the poing to the staid a quite and a they was and and and the got anything and Harry and Ron and Harry just and But was Malfoy, and the just and Ron and care the Gryffinding and sile poing the pull and and the pointing Ron and Harry just and But was Malfoy, and the just an\n",
      "15:47:10 End of epoch 69 , loss= 1.7671395540237427\n",
      " stared the for Malfoy, and the poing them. \n",
      "\n",
      "\"I'm said a suddenly, the was of the got they stard the for be the come some and the started a bund the poing the pointing the pointion. \n",
      "\n",
      "\"What's going the poing to start the Stone pall all they and a strumbledore the pointion. \n",
      "\n",
      "\"All I said Ron, and th\n",
      "15:47:25 End of epoch 70 , loss= 1.778382658958435\n",
      " stared the for Malfoy, and the purst and as the pointing anything a stopped and the the points of and the poing the points and them. \n",
      "\n",
      "\"I'm said Harry, said Harry, you just as door the for Malfoy, Malfoy, and was and the stard the pointing to studdenly and the for Malfoy, stance, and sunned and sup\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======================= Training ======================= #\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print( \"Running on\", device )\n",
    "\n",
    "config = Config()\n",
    "training_dataset = CharDataset('HP_book_1.txt', 32)\n",
    "print( \"There are\", len(training_dataset), \"datapoints and\", len(id_to_char), \"unique characters in the dataset\" ) \n",
    "training_loader = DataLoader(training_dataset, batch_size=config.batch_size)\n",
    "\n",
    "charlm = CharLM( config, len(id_to_char)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "charlm_optimizer = optim.Adam( charlm.parameters(), lr=config.learning_rate )\n",
    "\n",
    "charlm.train()\n",
    "print( datetime.now().strftime(\"%X\"), \"Training starts\" )\n",
    "for epoch in range(config.no_of_epochs) :\n",
    "    iteration = 0\n",
    "    for input_tensor, label in training_loader :\n",
    "        input_tensor, label = input_tensor.to(device), label.to(device)\n",
    "        charlm_optimizer.zero_grad()\n",
    "        logits = charlm(input_tensor).to(device)\n",
    "        loss = criterion(logits.squeeze(1), label)\n",
    "        loss.backward()\n",
    "        charlm_optimizer.step()\n",
    "        iteration += 1\n",
    "\n",
    "    print( datetime.now().strftime(\"%X\"), \"End of epoch\", epoch+1, \", loss=\", loss.detach().item())\n",
    "    charlm.eval()\n",
    "    # Generate 50 characters starting from the input text\n",
    "    try :\n",
    "        char_list = list(\"he took out his wand and\"[-MAXLEN:])\n",
    "        for i in range(300) :\n",
    "            input_tensor = torch.tensor( [char_to_id[c] for c in char_list] + [char_to_id[PADDING_SYMBOL]]*(MAXLEN-len(char_list))).unsqueeze(0).to(device)\n",
    "            logits = charlm(input_tensor).squeeze().to(device)\n",
    "            _, new_character_tensor = logits.topk(1)\n",
    "            new_character = id_to_char[new_character_tensor.detach().item()]\n",
    "            print( new_character, end='' )\n",
    "            if len(char_list) == MAXLEN :\n",
    "                char_list.pop(0)\n",
    "            char_list.append( new_character )\n",
    "        print()\n",
    "    except KeyError :\n",
    "        continue\n",
    "    charlm.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac9b627-90e7-440e-8dae-920030e9b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== User interaction ==================== #\n",
    "\n",
    "while True:\n",
    "    text = input(\"> \").strip()\n",
    "    if text == \"\" :\n",
    "        continue\n",
    "    char_list = list(text[-MAXLEN:])\n",
    "    # Generate 50 characters starting from the input text\n",
    "    try :\n",
    "        for i in range(50) :\n",
    "            input_tensor = torch.tensor( [char_to_id[c] for c in char_list] + [char_to_id[PADDING_SYMBOL]]*(MAXLEN-len(char_list))).unsqueeze(0).to(device)\n",
    "            logits = charlm(input_tensor).squeeze().to(device)\n",
    "            _, new_character_tensor = logits.topk(1)\n",
    "            new_character = id_to_char[new_character_tensor.detach().item()]\n",
    "            print( new_character, end='' )\n",
    "            if len(char_list) == MAXLEN :\n",
    "                char_list.pop(0)\n",
    "            char_list.append( new_character )\n",
    "        print()\n",
    "    except KeyError :\n",
    "        continue\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
