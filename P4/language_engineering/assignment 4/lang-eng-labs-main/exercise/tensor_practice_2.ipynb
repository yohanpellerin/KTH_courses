{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7f712f8-daa3-4dbd-9360-5d9d932c490e",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "The point of the exercise is to construct a simple neural character model that can predict the (n+1)th character, given the n preceding characters.\n",
    "\n",
    "Usually, such language models operate on the word level, but we use a character model because it is simpler and quicker to train and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1aeaf0-3a25-4467-b4d0-61535b30bd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run this cell\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8b035e-b525-432a-855f-de109266da05",
   "metadata": {},
   "source": [
    "We need to map every type of input item (every character, in our case) to a unique ID number. Since we are not sure which characters will appear in our training text, we are going to create new IDs as we encounter new kinds of characters we haven't seen before.\n",
    "\n",
    "For instance, if the text begins \"Harry Potter\", we want to transform this into $[1, 2, 3, 3, 4, 5, 6, 7, 8, 8, 9, 3, ...]$, where \"H\" has ID 1, \"a\" is 2, \"r\" is 3, etc. (ID 0 is reserved for the special padding symbol, so we start numbering from 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e58f36-84ad-45a7-9260-8e714a1a2cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to init mappings from characters to IDs and back again\n",
    "char_to_id = {}  # Dictionary to store character-to-ID mapping\n",
    "id_to_char = []  # List to store characters in their ID ordering\n",
    "PADDING_SYMBOL = '<PAD>'\n",
    "char_to_id[PADDING_SYMBOL] = 0  # ID 0 is reserved for <PAD>\n",
    "id_to_char.append(PADDING_SYMBOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81d0246-7a35-446e-8cf6-33e130b545c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the missing parts in this function \n",
    "def string_to_ids(string) :\n",
    "    \"\"\"\n",
    "    Translate this string into a list of character IDs.\n",
    "    The IDs will be integers 1,2,..., and created as needed.\n",
    "    \"\"\"\n",
    "    chars_ids = []  # This list will hold the result \n",
    "\n",
    "    for char in string:\n",
    "        # YOUR CODE HERE\n",
    "    return chars_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e94e393-2e80-4980-8fe7-fc1b766a3408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify\n",
    "with open('HP_book_1.txt', 'r', encoding='utf-8') as f:\n",
    "    contents = f.read() \n",
    "chars_ids = string_to_ids(contents)\n",
    "\n",
    "print(chars_ids[0] == chars_ids[442737])\n",
    "print(chars_ids[2677] == chars_ids[7692])\n",
    "print(chars_ids[146466] == chars_ids[312762])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ebae1-9f82-4c44-8f36-2dc936f3f785",
   "metadata": {},
   "source": [
    "We now define a class 'CharDataset' that extends the predefined 'Dataset' class.\n",
    "\n",
    "The init function reads a training text, and slides over it, creating chunks $n$ characters long. These chunks will be our data points, and the corresponding $(n+1)$th character will be the label.\n",
    "\n",
    "For instance, if $n=4$, and the text begins \"Harry P\", which corresponds to the IDs $1,2,3,3,4,5,6$, then the first data point will be $[1,2,3,3]$ and its label is $4$, the second data point is $[2,3,3,4]$ with label $5$, and the third data point is $[3,3,4,5]$ with label $6$.\n",
    "\n",
    "To extend the 'Dataset' class, the CharDataset class has to implement the __len__ and __getitem__ methods, as seen below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d7c88-15d2-47ad-8242-4a94aa826ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the missing parts in this class definition\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    datapoints = []  # Each datapoint is a sequence of n characters\n",
    "    labels = []  # The corresponding label is the character that comes next\n",
    "\n",
    "    def __init__(self, file_path, n):\n",
    "        \"\"\"\n",
    "        'file_path' is the name of the training data file\n",
    "\n",
    "        'n' is the number of consecutive characters the model will look at\n",
    "        to predict which letter comes next\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            contents = f.read() \n",
    "        chars_ids = string_to_ids(contents)\n",
    "\n",
    "        # Go through the chars_ids and create data points and labels\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datapoints)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx % len(self.datapoints)\n",
    "        return torch.tensor(self.datapoints[idx]), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e73905b-05a7-4e62-9229-0bd6b90c490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify\n",
    "dataset = CharDataset('HP_book_1.txt', 4)\n",
    "\n",
    "d402,l402 = dataset[402]\n",
    "d4002,l4002 = dataset[4002] \n",
    "d40002,l40002 = dataset[40002]\n",
    "print(l402.item() == d4002[0].item())\n",
    "print(l4002.item() == d402[3].item())\n",
    "print(l402.item() == d40002[2].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71171f5-5c9c-4a6c-b5ab-f7112daba56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell. The function below will take care of the case of\n",
    "# sequences of unequal lengths.\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def char_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Pads sequences to the longest sequence in the batch.\n",
    "\n",
    "    'batch' is a list of tuples [(datapoint, label), ...]\n",
    "\n",
    "    Returns a tuple of:\n",
    "            - Padded datapoints as a tensor\n",
    "            - Labels as a tensor \n",
    "   \"\"\"\n",
    "    datapoints, labels = zip(*batch)  \n",
    "    padded_datapoints = pad_sequence(datapoints, batch_first=True)\n",
    "    return padded_datapoints, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254e6c95-d1bd-4383-8e54-ce0ec1158130",
   "metadata": {},
   "source": [
    "#### Create a neural network according to the following specification:\n",
    "\n",
    "The hyperparameters are:\n",
    "* n -- the number of characters to input (to predict character n+1)\n",
    "* h -- the number of neurons in the hidden layer\n",
    "* v -- the number of unique characters\n",
    "\n",
    "The network should have:\n",
    "1. an embedding layer, mapping character IDs to h-dimensional vectors\n",
    "2. a hidden layer with a linear transformation of size $(nh)\\times(nh)$, followed by a tanh application\n",
    "3. a final layer with a linear transformation of size $(nh)\\times v$\n",
    "\n",
    "The input to the forward pass is a tensor of character IDs $x$ of shape $(\\mathtt{batch\\_size} \\times n)$. The forward pass should:\n",
    "1. Map $x$ to a tensor of character embeddings of shape $(\\mathtt{batch\\_size} \\times n \\times h)$\n",
    "2. Reshape that tensor to shape $(\\mathtt{batch\\_size} \\times nh)$\n",
    "3. Apply the hidden layer (linear transformation and the tanh operation)\n",
    "4. Apply the final layer\n",
    "5. Return the result of the last operation\n",
    "\n",
    "Before starting the implementation, have a look at the documentation for:\n",
    " - `torch.nn.Embedding`\n",
    " - `torch.nn.Linear`\n",
    " - `torch.tanh`\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ba45c-1a18-4e81-8d7f-7ea3d9f33470",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n, h, v):\n",
    "        super(CharModel, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9fac41-39ba-4800-bec1-f42deac3856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify\n",
    "_ = torch.manual_seed(42)\n",
    "charlm = CharModel(4,64,81)\n",
    "logits = charlm(torch.tensor([[10,9,12,1],[1,2,3,4]]))\n",
    "torch.isclose(logits[0,-1], torch.tensor(-0.0521), atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7192cd-6e9a-4cb1-8840-ca5973658f0d",
   "metadata": {},
   "source": [
    "Next, we will train a model with n=8, i.e. the model will try to predict the 9th character based on the 8 preceding characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efb5e1f-1f4c-4d1f-aee1-1f1857f98aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose 'Run all cells' in the 'Run' menu to run this cell.\n",
    "_ = torch.manual_seed(21)\n",
    "\n",
    "# ===================== Hyperparameters ================== #\n",
    "\n",
    "n = 8\n",
    "batch_size = 64\n",
    "hidden_size = 64\n",
    "learning_rate = 0.00001\n",
    "number_of_epochs = 50\n",
    "\n",
    "# ======================= Training ======================= #\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print( \"Running on\", device )\n",
    "\n",
    "training_dataset = CharDataset('HP_book_1.txt', n)\n",
    "print( \"There are\", len(training_dataset), \"datapoints and\", len(id_to_char), \"unique characters in the dataset\" ) \n",
    "\n",
    "training_loader = DataLoader(training_dataset, batch_size=batch_size, collate_fn=char_collate_fn, shuffle=True)\n",
    "charlm = CharModel(n, hidden_size, len(char_to_id)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "charlm_optimizer = optim.Adam(charlm.parameters(), lr=learning_rate)\n",
    "\n",
    "charlm.train()\n",
    "print(datetime.now().strftime(\"%X\"), \"Training starts\")\n",
    "for epoch in range(number_of_epochs) :\n",
    "    for input_tensor, label in training_loader:\n",
    "        input_tensor, label = input_tensor.to(device), label.to(device)\n",
    "        charlm_optimizer.zero_grad()\n",
    "        logits = charlm(input_tensor).to(device)\n",
    "        loss = criterion(logits.squeeze(1), label)\n",
    "        loss.backward()\n",
    "        charlm_optimizer.step()\n",
    "    print( datetime.now().strftime(\"%X\"), \"End of epoch\", epoch+1, \", loss=\", loss.detach().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02427a50-617e-476b-b3b3-ddfbac543286",
   "metadata": {},
   "source": [
    "Check how well the model works by entering a string and letting the model generate the continuation of that string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a3903-5bc9-4a8e-8299-fcb54f037f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "charlm.eval()\n",
    "while True :\n",
    "    start = input(\">\")\n",
    "    if start.strip() == 'quit' :\n",
    "        break\n",
    "    # Add spaces in case the start string is too short\n",
    "    start = ' '*(n-len(start)) + start\n",
    "    # Ignore everything but the last n characters of the start string\n",
    "    ids = [char_to_id[c] for c in start][-n:]\n",
    "    # Generate 200 characters starting from the start string\n",
    "    try:\n",
    "        for _ in range(200):\n",
    "            input_tensor = torch.tensor(ids).unsqueeze(0).to(device)\n",
    "            logits = charlm(input_tensor).squeeze().to(device)\n",
    "            _, new_character_tensor = logits.topk(1)\n",
    "            new_character_id = new_character_tensor.detach().item()\n",
    "            print(id_to_char[new_character_id], end='')\n",
    "            ids.pop(0)\n",
    "            ids.append(new_character_id)\n",
    "        print()\n",
    "    except KeyError:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
