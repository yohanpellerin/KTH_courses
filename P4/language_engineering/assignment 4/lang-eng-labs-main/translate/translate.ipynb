{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25306ba6-bbef-4762-b03c-09204d3c74ac",
   "metadata": {},
   "source": [
    "# Assignment 4, task 2\n",
    "Recurrent neural networks, particularly Gated Recurrent Units, were developed with a translation task in mind. We will now do a small English-to-Swedish translation experiment by training on a corpus from Tatoeba (https://tatoeba.org/en/).\n",
    "\n",
    "To this end, we are going to use an Encoder-Decoder architecture. The encoder is a recurrent neural network with GRU cells, that encodes the English input sentence. The Encoder can be either uni-directional or bi-directional. \n",
    "\n",
    "After the encoder has processed the English sentence, the decoder then takes over and generates the Swedish sentence, starting from the final hidden state of the encoder (or the concatenation of the two final hidden states, in the case of a bi-directional encoder). If an attention mechanism is used, the decoder will access all the hidden states of the encoder when deciding which word to output next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccda08cf-563d-46f7-8eb2-9c20f3c43a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: terminaltables in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "# First run this cell\n",
    "!pip install terminaltables\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "import codecs\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from terminaltables import AsciiTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0146c189-c61f-481f-9e02-970e95ac68f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mappings between symbols and integers, and vice versa.\n",
    "# They are global for all datasets.\n",
    "source_w2i = {}\n",
    "source_i2w = []\n",
    "target_w2i = {}\n",
    "target_i2w = []\n",
    "\n",
    "# The padding symbol will be used to ensure that all tensors in a batch\n",
    "# have equal length.\n",
    "PADDING_SYMBOL = ' '\n",
    "source_w2i[PADDING_SYMBOL] = 0\n",
    "source_i2w.append( PADDING_SYMBOL )\n",
    "target_w2i[PADDING_SYMBOL] = 0\n",
    "target_i2w.append( PADDING_SYMBOL )\n",
    "\n",
    "START_SYMBOL = '<START>'\n",
    "END_SYMBOL = '<END>'\n",
    "UNK_SYMBOL = '<UNK>'\n",
    "source_w2i[START_SYMBOL] = 1\n",
    "source_i2w.append( START_SYMBOL )\n",
    "target_w2i[START_SYMBOL] = 1\n",
    "target_i2w.append( START_SYMBOL )\n",
    "source_w2i[END_SYMBOL] = 2\n",
    "source_i2w.append( END_SYMBOL )\n",
    "target_w2i[END_SYMBOL] = 2\n",
    "target_i2w.append( END_SYMBOL )\n",
    "source_w2i[UNK_SYMBOL] = 3\n",
    "source_i2w.append( UNK_SYMBOL )\n",
    "target_w2i[UNK_SYMBOL] = 3\n",
    "target_i2w.append( UNK_SYMBOL )\n",
    "\n",
    "# Max number of words to be predicted if <END> symbol is not reached\n",
    "MAX_PREDICTIONS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c52eda7-56cc-49c9-a34c-36985fb666de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(embedding_file) :\n",
    "    \"\"\"\n",
    "    Reads pre-made embeddings from a file\n",
    "    \"\"\"\n",
    "    N = len(source_w2i)\n",
    "    embeddings = [0]*N\n",
    "    with codecs.open(embedding_file, 'r', 'utf-8') as f:\n",
    "        for line in f:\n",
    "            data = line.split()\n",
    "            word = data[0].lower()\n",
    "            if word not in source_w2i:\n",
    "                source_w2i[word] = N\n",
    "                source_i2w.append(word)\n",
    "                N += 1\n",
    "                embeddings.append(0)\n",
    "            vec = [float(x) for x in data[1:]]\n",
    "            D = len(vec)\n",
    "            embeddings[source_w2i[word]] = vec\n",
    "    # Add a '0' embedding for the padding symbol\n",
    "    embeddings[0] = [0]*D\n",
    "    # Check if there are words that did not have a ready-made Glove embedding\n",
    "    # For these words, add a random vector\n",
    "    for word in source_w2i :\n",
    "        index = source_w2i[word]\n",
    "        if embeddings[index] == 0 :\n",
    "            embeddings[index] = (np.random.random(D)-0.5).tolist()\n",
    "    return D, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45ff9d9f-fb01-46cc-83c9-466a0bbb3acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset) :\n",
    "    \"\"\"\n",
    "    A dataset with source sentences and their respective translations\n",
    "    into the target language.\n",
    "\n",
    "    Each sentence is represented as a list of word IDs. \n",
    "    \"\"\"\n",
    "    def __init__( self, filename, record_symbols=True ) :\n",
    "        try :\n",
    "            nltk.word_tokenize(\"hi there.\")\n",
    "        except LookupError:\n",
    "            nltk.download('punkt')\n",
    "        self.source_list = []\n",
    "        self.target_list = []\n",
    "        # Read the datafile\n",
    "        with codecs.open(filename, 'r', 'utf-8') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            for line in lines :\n",
    "                if '\\t' not in line :\n",
    "                    continue\n",
    "                s,t = line.split('\\t')\n",
    "                source_sentence = []\n",
    "                for w in nltk.word_tokenize(s) :\n",
    "                    if w not in source_i2w and record_symbols :\n",
    "                        source_w2i[w] = len(source_i2w)\n",
    "                        source_i2w.append( w )\n",
    "                    source_sentence.append( source_w2i.get(w, source_w2i[UNK_SYMBOL]) )\n",
    "                source_sentence.append(source_w2i[END_SYMBOL])\n",
    "                self.source_list.append( source_sentence )\n",
    "                target_sentence = []\n",
    "                for w in nltk.word_tokenize(t) :\n",
    "                    if w not in target_i2w and record_symbols :\n",
    "                        target_w2i[w] = len(target_i2w)\n",
    "                        target_i2w.append( w )\n",
    "                    target_sentence.append( target_w2i.get(w, target_w2i[UNK_SYMBOL]) )\n",
    "                target_sentence.append(target_w2i[END_SYMBOL])\n",
    "                self.target_list.append( target_sentence )\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.source_list)\n",
    "\n",
    "    def __getitem__(self, idx) :\n",
    "        return self.source_list[idx], self.target_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "920bdc47-f44b-4180-98f4-2ea1b18892c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell. The function below will take care of the case of\n",
    "# sequences of unequal lengths.\n",
    "\n",
    "def pad_sequence(batch, pad_source=source_w2i[PADDING_SYMBOL], pad_target=target_w2i[PADDING_SYMBOL]):\n",
    "    source, target = zip(*batch)\n",
    "    max_source_len = max(map(len, source))\n",
    "    max_target_len = max(map(len, target))\n",
    "    padded_source = [[b[i] if i < len(b) else pad_source for i in range(max_source_len)] for b in source]\n",
    "    padded_target = [[l[i] if i < len(l) else pad_target for i in range(max_target_len)] for l in target]\n",
    "    return padded_source, padded_target\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ba3edb-3ff2-4071-a24a-97e52a55a921",
   "metadata": {},
   "source": [
    "Here is the implementation of the encoder. For task 2(a), you will need to fill a part of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d526e15-f4a3-4d73-861a-fc878773d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Encoder ==================== #\n",
    "\n",
    "class EncoderRNN(nn.Module) :\n",
    "    \"\"\"\n",
    "    Encodes a batch of source sentences. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, no_of_input_symbols, embeddings=None, embedding_size=16, hidden_size=25,\n",
    "        encoder_bidirectional=False, device='cpu', use_gru=False, tune_embeddings=False) :\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.is_bidirectional = encoder_bidirectional\n",
    "        self.embedding = nn.Embedding(no_of_input_symbols,embedding_size)\n",
    "        if embeddings !=  None :\n",
    "            self.embedding.weight = nn.Parameter( torch.tensor(embeddings, dtype=torch.float), requires_grad=tune_embeddings )\n",
    "        if use_gru:\n",
    "            self.rnn = nn.GRU(embedding_size, hidden_size, batch_first=True, bidirectional=self.is_bidirectional)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(embedding_size, hidden_size, batch_first=True, bidirectional=self.is_bidirectional)\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def set_embeddings(self, embeddings):\n",
    "        self.embedding.weight = torch.tensor(embeddings, dtype=torch.float)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x is a list of lists of size (batch_size,max_seq_length)\n",
    "        Each inner list contains word IDs and represents one sentence.\n",
    "        The whole list-of-lists represents a batch of sentences.\n",
    "       \n",
    "        Returns:\n",
    "        the output from the encoder RNN: a pair of two tensors, one containing all hidden states, and one \n",
    "        containing the last hidden state (see https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n",
    "        \"\"\"\n",
    "\n",
    "        x_tensor = torch.tensor(x).to(self.device)\n",
    "        \n",
    "        # FOR TASK (a), REPLACE THE FOLLOWING LINE WITH YOUR CODE\n",
    "        embedded = self.embedding(x_tensor)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40c6219-14aa-483b-ad15-46db4e6705af",
   "metadata": {},
   "source": [
    "Here is the decoder. For tasks (b) and (c), fill in the missing code in the 'forward' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9962c2c-b5d6-49fc-b2e4-f9e20a914fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Decoder ==================== #\n",
    "\n",
    "class DecoderRNN(nn.Module) :\n",
    "\n",
    "    def __init__(self, no_of_output_symbols, embedding_size=16, hidden_size=25, use_attention=True,\n",
    "        display_attention=False, device='cpu', use_gru=False) :\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(no_of_output_symbols,embedding_size)\n",
    "        self.no_of_output_symbols = no_of_output_symbols\n",
    "        self.W = nn.Parameter(torch.rand(hidden_size, hidden_size)-0.5) # shouldn't W be 2*hidden_size\n",
    "        self.U = nn.Parameter(torch.rand(hidden_size, hidden_size)-0.5)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size, 1)-0.5)\n",
    "        self.use_attention = use_attention\n",
    "        self.display_attention = display_attention\n",
    "        if use_gru:\n",
    "            self.rnn = nn.GRU(embedding_size, hidden_size, batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(embedding_size, hidden_size, batch_first=True)\n",
    "        self.output = nn.Linear( hidden_size, no_of_output_symbols )\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inp, hidden, encoder_outputs) :\n",
    "        \"\"\"\n",
    "        'input' is a list of length batch_size, containing the current word\n",
    "        of each sentence in the batch\n",
    "\n",
    "        'hidden' is a tensor containing the last hidden state of the decoder, \n",
    "        for each sequence in the batch\n",
    "        hidden.shape = (1, batch_size, hidden_size)\n",
    "\n",
    "        'encoder_outputs' is a tensor containing all hidden states from the\n",
    "        encoder (used in problem c)\n",
    "        encoder_outputs.shape = (batch_size, max_seq_length, hidden_size)\n",
    "\n",
    "        Note that 'max_seq_length' above refers to the max_seq_length\n",
    "        of the encoded sequence (not the decoded sequence).\n",
    "\n",
    "        Returns:\n",
    "        If use_attention and display_attention are both True (task (c)), return a triple\n",
    "        (logits for the predicted next word, hidden state, attention weights alpha)\n",
    "\n",
    "        Otherwise (task (b)), return a pair\n",
    "        (logits for the predicted next word, hidden state).\n",
    "        \"\"\"\n",
    "        inp_tensor = torch.tensor(inp).to(self.device)\n",
    "        batch_size = len(inp_tensor)\n",
    "        inp_embedded = self.embedding(inp_tensor).reshape(batch_size, 1, self.embedding.embedding_dim)\n",
    "        # FOR (b) and (c) REPLACE THE FOLLOWING LINE WITH YOUR CODE\n",
    "\n",
    "        if self.use_attention:\n",
    "            # FOR TASK (c), REPLACE THE FOLLOWING LINE WITH YOUR CODE\n",
    "            \n",
    "            sum_1 = encoder_outputs @ self.W\n",
    "            sum_2 = hidden @ self.U\n",
    "            #recopie sum_2 pour chaque mot de la sequence\n",
    "            sum_2 = sum_2.repeat(encoder_outputs.shape[1],1,1)\n",
    "            sum_2 = sum_2.reshape(batch_size,encoder_outputs.shape[1],encoder_outputs.shape[2])\n",
    "            e_matrix = torch.tanh(sum_1 + sum_2) @ self.v\n",
    "            alpha_matrix = F.softmax(e_matrix, dim=1)\n",
    "            context_matrix = alpha_matrix * encoder_outputs\n",
    "            context_matrix = torch.sum(context_matrix, dim=1)\n",
    "            context_matrix = context_matrix.reshape(1,context_matrix.shape[0],-1)\n",
    "            output, hidden = self.rnn(inp_embedded, context_matrix)\n",
    "            logits = self.output(hidden).reshape(batch_size,-1)\n",
    "            return logits, hidden, alpha_matrix\n",
    "        else:\n",
    "            _, hidden = self.rnn(inp_embedded, hidden)\n",
    "            logits = self.output(hidden).reshape(batch_size,-1)\n",
    "            return logits, hidden\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7835bb5-a9db-4cb6-b044-1a9b0581ff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be used for evaluation of both the dev set (during training)\n",
    "# and the test set (after training is finished).\n",
    "def evaluate(ds, encoder, decoder):\n",
    "    confusion = [[0 for a in target_i2w] for b in target_i2w] \n",
    "    correct_sentences, incorrect_sentences = 0,0\n",
    "    for x, y in ds :\n",
    "        predicted_sentence = []\n",
    "        outputs, hidden = encoder( [x] )\n",
    "        if encoder.is_bidirectional :\n",
    "            hidden = hidden.permute((1,0,2)).reshape(1,-1).unsqueeze(0)\n",
    "        predicted_symbol = target_w2i[START_SYMBOL]\n",
    "        for correct in y :\n",
    "            predictions, hidden = decoder( [predicted_symbol], hidden, outputs )\n",
    "            _, predicted_tensor = predictions.topk(1)\n",
    "            predicted_symbol = predicted_tensor.detach().item()\n",
    "            confusion[int(predicted_symbol)][int(correct)] += 1\n",
    "            predicted_sentence.append( predicted_symbol )\n",
    "        if predicted_sentence == y :\n",
    "            correct_sentences += 1\n",
    "        else :\n",
    "            incorrect_sentences += 1\n",
    "    correct_symbols = sum( [confusion[i][i] for i in range(len(confusion))] )\n",
    "    all_symbols = torch.tensor(confusion).sum().item()    \n",
    "\n",
    "    # Construct a neat confusion matrix\n",
    "    for i in range(len(confusion)) :\n",
    "        confusion[i].insert(0,target_i2w[i])\n",
    "    first_row = [\"Predicted/Real\"]\n",
    "    first_row.extend(target_i2w)\n",
    "    confusion.insert(0,first_row)\n",
    "    # t = AsciiTable( confusion )\n",
    "    \n",
    "    #print( t.table )\n",
    "    print( \"Correctly predicted words    : \", correct_symbols )\n",
    "    print( \"Incorrectly predicted words  : \", all_symbols-correct_symbols )\n",
    "    print( \"Correctly predicted sentences  : \", correct_sentences )\n",
    "    print( \"Incorrectly predicted sentences: \", incorrect_sentences )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6466f4-b193-4b83-b88b-f492b4429e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 'Run all cells' to do the training.\n",
    "\n",
    "# ================ Hyper-parameters ================ #\n",
    "\n",
    "use_attention = True     \n",
    "use_gru = True         # Use Gated Recurrent Units (rather than plain RNNs)\n",
    "bidirectional = True   # Use a bidirectional encoder\n",
    "use_embeddings = True      # Use pre-loaded Glove embeddings\n",
    "tune_embeddings = True # Fine-tune the Glove embeddings\n",
    "batch_size = 64\n",
    "hidden_size = 25       # Number of dimensions in the hidden state\n",
    "learning_rate = 0.001\n",
    "epochs = 50            # We will train for this many epochs\n",
    "save = False           # Do not save the model\n",
    "\n",
    "# ====================== Data ===================== #\n",
    "\n",
    "training_file = '/datasets/dd2417/eng-swe-train.txt'\n",
    "test_file = '/datasets/dd2417/eng-swe-test.txt'\n",
    "dev_file = '/datasets/dd2417/eng-swe-dev.txt'\n",
    "\n",
    "# ==================== Training ==================== #\n",
    "# Reproducibility\n",
    "# Read a bit more here -- https://pytorch.org/docs/stable/notes/randomness.html\n",
    "random.seed(5719)\n",
    "np.random.seed(5719)\n",
    "#torch.manual_seed(5719)\n",
    "#torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# Can we run on GPU?\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current device: {}\".format(torch.cuda.get_device_name(0)))\n",
    "else:\n",
    "    print('Running on CPU')\n",
    "print()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Read datasets\n",
    "training_dataset = TranslationDataset(training_file)\n",
    "dev_dataset = TranslationDataset(dev_file, record_symbols=False )\n",
    "\n",
    "print( \"Number of source words: \", len(source_i2w) )\n",
    "print( \"Number of target words: \", len(target_i2w) )\n",
    "print( \"Number of training sentences: \", len(training_dataset) )\n",
    "print()\n",
    "\n",
    "# If we have pre-computed word embeddings, then make sure these are used\n",
    "if use_embeddings:\n",
    "    embedding_size, embeddings = load_glove_embeddings('/datasets/dd2417/glove.6B.50d.txt')\n",
    "else :\n",
    "    embedding_size = args.hidden_size\n",
    "    embeddings = None\n",
    "\n",
    "training_loader = DataLoader(training_dataset, batch_size=batch_size, collate_fn=pad_sequence)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, collate_fn=pad_sequence)\n",
    "        \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "encoder = EncoderRNN(\n",
    "    len(source_i2w),\n",
    "    embeddings=embeddings,\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_size=hidden_size,\n",
    "    encoder_bidirectional=bidirectional,\n",
    "    tune_embeddings=tune_embeddings,\n",
    "    use_gru=use_gru,\n",
    "    device=device\n",
    ")\n",
    "decoder = DecoderRNN(\n",
    "    len(target_i2w),\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_size=hidden_size*(bidirectional+1),\n",
    "    use_attention=use_attention,\n",
    "    use_gru=use_gru,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "print( datetime.now().strftime(\"%H:%M:%S\"), \"Starting training.\" )\n",
    "\n",
    "for epoch in range( epochs ) :\n",
    "    total_loss = 0\n",
    "    for source, target in training_loader: #tqdm(training_loader, desc=\"Epoch {}\".format(epoch + 1)):\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        # hidden is (D * num_layers, B, H)\n",
    "        outputs, hidden = encoder( source )\n",
    "        if bidirectional:\n",
    "            # (2, B, H) -> (B, 2 * H) -> (1, B, 2 * H)\n",
    "            hidden = torch.cat([hidden[0,:, :], hidden[1,:,:]], dim=1).unsqueeze(0)\n",
    "                    \n",
    "        # The probability of doing teacher forcing will decrease\n",
    "        # from 1 to 0 over the range of epochs. This could be implemented\n",
    "        # like this:\n",
    "        # teacher_forcing_ratio = 1- epoch/args.epochs\n",
    "        # But, for now we will always use teacher forcing\n",
    "        teacher_forcing_ratio = 1\n",
    "\n",
    "        # The input to the decoder in the first time step will be\n",
    "        # the boundary symbol, regardless if we are using teacher\n",
    "        # forcing or not.\n",
    "        idx = [target_w2i[START_SYMBOL] for sublist in target]\n",
    "        predicted_symbol = [target_w2i[START_SYMBOL] for sublist in target]\n",
    "\n",
    "        target_length = len(target[0])\n",
    "        for i in range(target_length) :\n",
    "            use_teacher_forcing = (random.random() < teacher_forcing_ratio)\n",
    "            if use_teacher_forcing :\n",
    "                predictions, hidden = decoder( idx, hidden, outputs )\n",
    "            else:\n",
    "                # Here we input the previous prediction rather than the\n",
    "                # correct symbol.\n",
    "                predictions, hidden = decoder( predicted_symbol, hidden, outputs )\n",
    "            _, predicted_tensor = predictions.topk(1)\n",
    "            predicted_symbol = predicted_tensor.squeeze().tolist()\n",
    "\n",
    "            # The targets will be the ith symbol of all the target\n",
    "            # strings. They will also be used as inputs for the next\n",
    "            # time step if we use teacher forcing.\n",
    "            idx = [sublist[i] for sublist in target]\n",
    "            loss += criterion( predictions.squeeze(), torch.tensor(idx).to(device) )\n",
    "        loss /= (target_length * batch_size)\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        total_loss += loss\n",
    "    print( datetime.now().strftime(\"%H:%M:%S\"), \"Epoch\", epoch, \"loss:\", total_loss.detach().item() )\n",
    "    total_loss = 0\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Evaluating on the dev data...\")\n",
    "        evaluate(dev_dataset, encoder, decoder)\n",
    "\n",
    "# ==================== Save the model  ==================== #\n",
    "\n",
    "if ( save ) :\n",
    "    dt = str(datetime.now()).replace(' ','_').replace(':','_').replace('.','_')\n",
    "    newdir = 'model_' + dt\n",
    "    os.mkdir( newdir )\n",
    "    torch.save( encoder.state_dict(), os.path.join(newdir, 'encoder.model') )\n",
    "    torch.save( decoder.state_dict(), os.path.join(newdir, 'decoder.model') )\n",
    "    with open( os.path.join(newdir, 'source_w2i'), 'wb' ) as f :\n",
    "        pickle.dump( source_w2i, f )\n",
    "        f.close()\n",
    "    with open( os.path.join(newdir, 'source_i2w'), 'wb' ) as f :\n",
    "        pickle.dump( source_i2w, f )\n",
    "        f.close()\n",
    "    with open( os.path.join(newdir, 'target_w2i'), 'wb' ) as f :\n",
    "        pickle.dump( target_w2i, f )\n",
    "        f.close()\n",
    "    with open( os.path.join(newdir, 'target_i2w'), 'wb' ) as f :\n",
    "        pickle.dump( target_i2w, f )\n",
    "        f.close()\n",
    "\n",
    "    settings = {\n",
    "        'training_set': training_file,\n",
    "        'test_set': test_file,\n",
    "        'epochs': epochs,\n",
    "        'learning_rate': learning_rate,\n",
    "        'batch_size': batch_size,\n",
    "        'hidden_size': hidden_size,\n",
    "        'attention': attention,\n",
    "        'bidirectional': bidirectional,\n",
    "        'embedding_size': embedding_size,\n",
    "        'use_gru': use_gru,\n",
    "        'tune_embeddings': tune_embeddings\n",
    "    }\n",
    "    with open( os.path.join(newdir, 'settings.json'), 'w' ) as f:\n",
    "        json.dump(settings, f)\n",
    "\n",
    "# ==================== Evaluation ==================== #\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "print( \"Evaluating on the test data...\" )\n",
    "\n",
    "test_dataset = TranslationDataset( test_file, record_symbols=False )\n",
    "print( \"Number of test sentences: \", len(test_dataset) )\n",
    "print()\n",
    "\n",
    "evaluate(test_dataset, encoder, decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607a61f1-fe2c-4806-9387-cc7811d14489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== User interaction ==================== #\n",
    "\n",
    "decoder.display_attention = True\n",
    "while( True ) :\n",
    "    text = input( \"> \" )\n",
    "    if text == \"\" :\n",
    "        continue\n",
    "    try :\n",
    "        source_sentence = [source_w2i[w] for w in nltk.word_tokenize(text)]\n",
    "    except KeyError :\n",
    "        print( \"Erroneous input string\" )\n",
    "        continue\n",
    "    outputs, hidden = encoder( [source_sentence] )\n",
    "    if encoder.is_bidirectional :\n",
    "        hidden = hidden.permute((1,0,2)).reshape(1,-1).unsqueeze(0)\n",
    "        \n",
    "    predicted_symbol = target_w2i[START_SYMBOL]\n",
    "    target_sentence = []\n",
    "    attention_probs = []    \n",
    "    num_attempts = 0\n",
    "    while num_attempts < MAX_PREDICTIONS:\n",
    "        if use_attention :\n",
    "            predictions, hidden, alpha = decoder( [predicted_symbol], hidden, outputs )\n",
    "            attention_probs.append( alpha.permute(0,2,1).squeeze().detach().tolist() )\n",
    "        else :\n",
    "            predictions, hidden = decoder( [predicted_symbol], hidden, outputs )\n",
    "            \n",
    "        _, predicted_tensor = predictions.topk(1)\n",
    "        predicted_symbol = predicted_tensor.detach().item()\n",
    "        target_sentence.append( predicted_symbol )\n",
    "\n",
    "        num_attempts += 1\n",
    "\n",
    "        if predicted_symbol == target_w2i[END_SYMBOL] :\n",
    "            break\n",
    "\n",
    "    for i in target_sentence :\n",
    "        print( target_i2w[i].encode('utf-8').decode(), end=' ' )\n",
    "    print()\n",
    "\n",
    "    if use_attention :\n",
    "        # Construct the attention table\n",
    "        ap = torch.tensor(attention_probs).T\n",
    "        if len(ap.shape) == 1:\n",
    "            ap = ap.unsqueeze(0)\n",
    "        attention_probs = ap.tolist()\n",
    "            \n",
    "        for i in range(len(attention_probs)) :\n",
    "            for j in range(len(attention_probs[i])) :\n",
    "                attention_probs[i][j] = \"{val:.2f}\".format(val=attention_probs[i][j])\n",
    "        for i in range(len(attention_probs)) :\n",
    "            if i<len(text) :\n",
    "                attention_probs[i].insert(0,source_i2w[source_sentence[i]])\n",
    "            else :\n",
    "                attention_probs[i].insert(0,' ')\n",
    "        first_row = [\"Source/Result\"]\n",
    "        for w in target_sentence :\n",
    "            first_row.append(target_i2w[w])\n",
    "        attention_probs.insert(0,first_row)\n",
    "        t = AsciiTable( attention_probs )\n",
    "        print( t.table )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
